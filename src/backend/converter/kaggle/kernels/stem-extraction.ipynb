{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":7074304,"sourceType":"datasetVersion","datasetId":4071546},{"sourceId":7715355,"sourceType":"datasetVersion","datasetId":4505888},{"sourceId":7722481,"sourceType":"datasetVersion","datasetId":4505910}],"dockerImageVersionId":30648,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!python --version\n!pip install torch  --index-url https://download.pytorch.org/whl/cu118\n!pip install matplotlib\n!pip install soundfile\n!pip install librosa audioread\n!pip install mir_eval","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-03-18T08:30:19.276665Z","iopub.execute_input":"2024-03-18T08:30:19.277012Z","iopub.status.idle":"2024-03-18T08:30:20.918330Z","shell.execute_reply.started":"2024-03-18T08:30:19.276987Z","shell.execute_reply":"2024-03-18T08:30:20.915277Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport re\nimport numpy as np\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nimport torch.nn as nn\nimport torch.optim as optim\nimport librosa\nimport librosa.display\nimport torch.nn.functional as F\nimport datetime\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport soundfile as sf\nimport mir_eval\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\n\nimport gc\ntorch.cuda.empty_cache()\ngc.collect()\n\ndevice = (\n    \"cuda\"\n    if torch.cuda.is_available()\n    else \"mps\"\n    if torch.backends.mps.is_available()\n    else \"cpu\"\n)","metadata":{"execution":{"iopub.status.busy":"2024-03-18T08:30:20.919582Z","iopub.status.idle":"2024-03-18T08:30:20.919929Z","shell.execute_reply.started":"2024-03-18T08:30:20.919755Z","shell.execute_reply":"2024-03-18T08:30:20.919775Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_path = \"/kaggle/input/musdb18-hq/test\"\ntrain_path = \"/kaggle/input/musdb18-hq/train\"\n\nfiles = {\"train_x\":[], \"train_y\":[], \"test_x\":[], \"test_y\":[]}\n\nprint(\"Processing specto for \"+train_path)\nfor root, dirs, _ in os.walk(train_path):\n    dirs.sort()\n    for d in dirs :\n        for r2,d2,f2 in os.walk(os.path.join(root, d)):\n            f2.sort()\n            files[\"train_x\"]+=[[os.path.join(r2, \"mixture.wav\")]]\n            files[\"train_y\"]+=[[os.path.join(r2, name) for name in f2 if \"mixture\" not in name]]\n                \n\nprint(\"Processing specto for \"+test_path)\nfor root, dirs, _ in os.walk(test_path):\n    dirs.sort()\n    for d in dirs :\n        for r2,d2,f2 in os.walk(os.path.join(root, d)):\n            f2.sort()\n            files[\"test_x\"]+=[[os.path.join(r2, \"mixture.wav\")]]\n            files[\"test_y\"]+=[[os.path.join(r2, name) for name in f2 if \"mixture\" not in name]]\n#print(\"all spectos saved\")\n#print((max_freq,max_tframe))\nfiles[\"train_x\"]+=files[\"test_x\"][:(len(files[\"test_x\"])//3)]\nfiles[\"train_y\"]+=files[\"test_y\"][:(len(files[\"test_y\"])//3)]\nfiles[\"test_x\"]=files[\"test_y\"][(len(files[\"test_x\"])//3):]\nfiles[\"test_y\"]=files[\"test_y\"][(len(files[\"test_y\"])//3):]\nfor f in files.keys() :\n  print(f)\n  print(len(files[f]))","metadata":{"execution":{"iopub.status.busy":"2024-03-18T08:30:20.922229Z","iopub.status.idle":"2024-03-18T08:30:20.922679Z","shell.execute_reply.started":"2024-03-18T08:30:20.922450Z","shell.execute_reply":"2024-03-18T08:30:20.922469Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"freq_bins = 1024\ntframe = 2048\nclass SpectroDataset(Dataset) :\n    def __init__(self, train_x, train_y):\n        self.train_x = train_x\n        self.train_y = train_y\n\n    def __len__(self):\n        return len(self.train_x)\n\n    def __getitem__(self, idx):\n        #print(self.train_x[idx])\n        #print(self.train_y[idx])\n        audio, _ = librosa.load(self.train_x[idx][0])\n        Sx = np.abs(librosa.stft(audio))\n        magnitude, _ = librosa.magphase(Sx)\n        mixture = torch.from_numpy(magnitude[:freq_bins,:tframe])\n        mixture = mixture.to(device)\n        to_pad = tframe-mixture.shape[1]\n        if to_pad>0 :\n          if to_pad%2==0 :\n              mixture = F.pad(mixture,(to_pad//2,to_pad//2),mode='constant',value=0)\n          else :\n              mixture = F.pad(mixture,(to_pad//2,to_pad//2+1),mode='constant',value=0)\n        stems = dict()\n        for s in self.train_y[idx] :\n            audio, _ = librosa.load(s)\n            Sy = np.abs(librosa.stft(audio))\n            magnitude, _ = librosa.magphase(Sy)\n            stem = torch.from_numpy(magnitude[:freq_bins,:tframe])\n            stem = stem.to(device)\n            to_pad = tframe-stem.shape[1]\n            if to_pad>0 :\n              if to_pad%2==0 :\n                  stem = F.pad(stem,(to_pad//2,to_pad//2),mode='constant',value=0)\n              else :\n                  stem = F.pad(stem,(to_pad//2,to_pad//2+1),mode='constant',value=0)\n            if \"bass\" in s :\n                stems[\"bass\"] = stem\n            elif \"drums\" in s :\n                stems[\"drums\"] = stem\n            elif \"other\" in s :\n                stems[\"other\"] = stem\n            elif \"vocal\" in s :\n                stems[\"vocal\"] = stem\n        return mixture, stems","metadata":{"execution":{"iopub.status.busy":"2024-03-18T08:30:20.923759Z","iopub.status.idle":"2024-03-18T08:30:20.924230Z","shell.execute_reply.started":"2024-03-18T08:30:20.923986Z","shell.execute_reply":"2024-03-18T08:30:20.924005Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class UNet(nn.Module) :\n    def __init__(self):\n        super().__init__()\n\n        self.relu = nn.ReLU()\n        self.softmax = nn.Softmax(dim=1)\n\n        # Encoder\n        self.e11 = nn.Conv2d(1, 16, kernel_size=3, padding=1)\n        self.e12 = nn.Conv2d(16, 16, kernel_size=3, padding=1)\n        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n\n        self.e21 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n        self.e22 = nn.Conv2d(32, 32, kernel_size=3, padding=1)\n        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n\n        self.e31 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n        self.e32 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n\n        #Bottleneck\n        self.e51 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n        self.e52 = nn.Conv2d(128, 128, kernel_size=3, padding=1)\n\n\n        # Decoder\n        self.upconv2 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n        self.d2 = nn.ConvTranspose2d(128, 64, kernel_size=3, padding=1)\n        self.bn2 = nn.BatchNorm2d(64)\n\n        self.upconv3 = nn.ConvTranspose2d(64, 32, kernel_size=2, stride=2)\n        self.d3 = nn.ConvTranspose2d(64, 32, kernel_size=3, padding=1)\n        self.bn3 = nn.BatchNorm2d(32)\n\n        self.upconv4 = nn.ConvTranspose2d(32, 16, kernel_size=2, stride=2)\n        self.d4 = nn.ConvTranspose2d(32, 16, kernel_size=3, padding=1)\n        self.bn4 = nn.BatchNorm2d(16)\n\n        # Output layer\n        self.outconv = nn.ConvTranspose2d(16, 4, kernel_size=1)\n        self.dropout = nn.Dropout(p=0.4)\n\n\n    def forward_down(self,x) :\n        # Encoder\n        xe11 = self.relu(self.e11(x))\n        xe12 = self.relu(self.e12(xe11))\n        xp1 = self.pool1(xe12)\n\n        xe21 = self.relu(self.e21(xp1))\n        xe22 = self.relu(self.e22(xe21))\n        xp2 = self.pool2(xe22)\n\n        xe31 = self.relu(self.e31(xp2))\n        xe32 = self.relu(self.e32(xe31))\n        xp3 = self.pool3(xe32)\n\n        #Bottleneck\n        xe51 = self.relu(self.e51(xp3))\n        xe52 = self.relu(self.e52(xe51))\n        return xe52, [xe32,xe22,xe12]\n\n    def forward_up(self,x,to_concats) :\n        # Decoder\n        xu2 = self.upconv2(x)\n        xu21 = torch.cat((xu2, to_concats[0]), dim=1)\n        xd2 = self.dropout(self.bn2(self.relu(self.d2(xu21))))\n\n        xu3 = self.upconv3(xd2)\n        xu31 = torch.cat((xu3, to_concats[1]), dim=1)\n        xd3 = self.dropout(self.bn3(self.relu(self.d3(xu31))))\n\n        xu4 = self.upconv4(xd3)\n        xu41 = torch.cat((xu4, to_concats[2]), dim=1)\n        xd4 = self.dropout(self.bn4(self.relu(self.d4(xu41))))\n\n        # Output\n        out = self.outconv(xd4)\n        out = self.dropout(out)\n        out = self.softmax(out)\n        return out\n\n    def forward(self, input):\n        x = input.unsqueeze(0)\n        x, to_concats = self.forward_down(x)\n\n        masks = self.forward_up(x,to_concats)\n        vocal_stem = masks[:,0,:,:] * input\n        bass_stem = masks[:,1,:,:] * input\n        drums_stem = masks[:,2,:,:] * input\n        other_stem = masks[:,3,:,:] * input\n\n        stems = {\"vocal_stem\":vocal_stem.squeeze(1),\n                 \"bass_stem\":bass_stem.squeeze(1),\n                 \"drums_stem\":drums_stem.squeeze(1),\n                 \"other_stem\":other_stem.squeeze(1)}\n        return stems","metadata":{"execution":{"iopub.status.busy":"2024-03-18T08:30:20.926066Z","iopub.status.idle":"2024-03-18T08:30:20.926534Z","shell.execute_reply.started":"2024-03-18T08:30:20.926297Z","shell.execute_reply":"2024-03-18T08:30:20.926315Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class MultiTaskLoss(nn.Module) :\n    def __init__(self, num_tasks):\n        super(MultiTaskLoss, self).__init__()\n        \n        self.weights = nn.Parameter(torch.ones(num_tasks, requires_grad=True).to(device))\n        self.loss_fn = nn.L1Loss()\n\n    def forward(self, predictions, targets):\n        total_loss = 0.0\n        for task_idx in range(len(predictions)):\n            task_loss = self.loss_fn(predictions[task_idx], targets[task_idx])\n            total_loss += self.weights[task_idx] * task_loss\n\n        return total_loss","metadata":{"execution":{"iopub.status.busy":"2024-03-18T08:30:20.928810Z","iopub.status.idle":"2024-03-18T08:30:20.929294Z","shell.execute_reply.started":"2024-03-18T08:30:20.929020Z","shell.execute_reply":"2024-03-18T08:30:20.929038Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = UNet().to(device)\nmulti_task_loss = MultiTaskLoss(4).to(device)\nloss_fn = nn.L1Loss()\noptimizer = optim.SGD(model.parameters(), lr=0.01)\nscheduler = ReduceLROnPlateau(optimizer, patience=5)\n\n'''\noptimizer = optim.Adam([{'params': model.parameters()}, \n                        {'params': multi_task_loss.parameters()}],\n                        lr=0.001, betas=(0.9, 0.999))\n\n\nstate_dict = torch.load(checkpoint_path)\nprint(\"loaded for resume training: \"+best_checkpoint_path)\nmodel.load_state_dict(state_dict['model_state_dict'])\nmulti_task_loss.load_state_dict(state_dict['loss_state_dict'])\noptimizer.load_state_dict(state_dict['optimizer_sitate_dict'])\n'''\n\ntimestamp = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')\nEPOCHS = 50\n\nbest_vloss = 1_000_000.\nbest_sdr = -1.0 \nbest_checkpoint_path = \"\"\n\n\ntrain_dataloader = DataLoader(SpectroDataset(files[\"train_x\"],files[\"train_y\"]), batch_size=1, shuffle=True)\ntest_dataloader = DataLoader(SpectroDataset(files[\"test_x\"],files[\"test_y\"]), batch_size=1, shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2024-03-18T08:30:20.931024Z","iopub.status.idle":"2024-03-18T08:30:20.931628Z","shell.execute_reply.started":"2024-03-18T08:30:20.931380Z","shell.execute_reply":"2024-03-18T08:30:20.931399Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_one_epoch(epoch_index):\n    running_loss = 0.\n    training_losses = []\n    avg_losses = []\n    T,K=2.,4.\n    for i, data in enumerate(train_dataloader):\n        # Every data instance is an input + label pair\n        mixtures, stems = data\n        zr = np.zeros((freq_bins,tframe))\n        if \"bass\" not in stems : \n            stems[\"bass\"] = torch.from_numpy(zr).to(device)\n        if \"vocal\" not in stems : \n            stems[\"vocal\"] = torch.from_numpy(zr).to(device)\n        if \"drums\" not in stems : \n            stems[\"drums\"] = torch.from_numpy(zr).to(device)\n        if \"other\" not in stems : \n            stems[\"other\"] = torch.from_numpy(zr).to(device)\n            \n        #for s in stems.keys() : stems[s] = stems[s].unsqueeze(0) \n        # Zero your gradients for every batch!\n        optimizer.zero_grad()\n\n        # Make predictions for this batch\n        outputs = model(mixtures)\n        #print(outputs[\"bass_stem\"].shape)\n        #print(stems[\"bass\"].shape)\n\n        # Compute the loss and its gradients\n        # Dynamic Weight Average strat\n        \n        y1 = torch.tensor(avg_losses[i-1][0] / avg_losses[i-2][0] if i>1 else 1.)\n        y2 = torch.tensor(avg_losses[i-1][1] / avg_losses[i-2][1] if i>1 else 1.)\n        y3 = torch.tensor(avg_losses[i-1][2] / avg_losses[i-2][2] if i>1 else 1.)\n        y4 = torch.tensor(avg_losses[i-1][3] / avg_losses[i-2][3] if i>1 else 1.)\n        y = torch.tensor([y1,y2,y3,y4])/T\n        softmx = F.softmax(y,dim=0)\n        zero_mask = softmx==0.0\n        softmx[zero_mask] = 1e-9\n        dwa = K * softmx\n        loss1=loss_fn(outputs[\"vocal_stem\"], stems[\"vocal\"])\n        loss2=loss_fn(outputs[\"bass_stem\"], stems[\"bass\"])\n        loss3=loss_fn(outputs[\"drums_stem\"], stems[\"drums\"])\n        loss4=loss_fn(outputs[\"other_stem\"], stems[\"other\"])\n        loss=dwa[0]*loss1+dwa[1]*loss2+dwa[2]*loss3+dwa[3]*loss4\n        weightless_loss=[loss1.item(),loss2.item(),loss3.item(),loss4.item()]\n        training_losses+=[weightless_loss]\n        current_avg = [sum(training_losses[x][k] for x in range(i+1))/(i+1) for k in range(4)]\n        print(\"dwa: \"+str(dwa))\n        print('weightless_loss: '+str(weightless_loss))\n        print(\"current_avg: \"+ str(current_avg))\n        current_avg = [x  if x!=0.0 else 1e-9 for x in current_avg]\n        avg_losses += [current_avg]\n        '''\n        loss = multi_task_loss([outputs[\"vocal_stem\"],outputs[\"bass_stem\"],outputs[\"drums_stem\"],outputs[\"other_stem\"]], \n                               [stems[\"vocal\"],stems[\"bass\"],stems[\"drums\"],stems[\"other\"]])\n        '''\n        loss.backward()\n\n        # Adjust learning weights\n        optimizer.step()\n\n        # Gather data and report\n        running_loss += loss.item()\n        print('training batch {}, loss: {}'.format(i + 1, loss.item()))\n\n    return running_loss/(i+1)","metadata":{"execution":{"iopub.status.busy":"2024-03-18T08:30:20.933107Z","iopub.status.idle":"2024-03-18T08:30:20.933573Z","shell.execute_reply.started":"2024-03-18T08:30:20.933345Z","shell.execute_reply":"2024-03-18T08:30:20.933364Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"avg_loss_history = []\nfor e in range(EPOCHS):\n    epoch = e+1\n    print('EPOCH {}:'.format(epoch))\n\n    # Make sure gradient tracking is on, and do a pass over the data\n    model.train(True)\n    avg_loss = train_one_epoch(epoch)\n    running_vloss = 0.0\n    # Set the model to evaluation mode, disabling dropout and using population\n    # statistics for batch normalization.\n    model.eval()\n    running_sdr = 0.0\n    # Disable gradient computation and reduce memory consumption.\n    with torch.no_grad():\n        testing_losses = []\n        avg_losses = []\n        T,K=2.,4.\n        for i, data in enumerate(test_dataloader):\n            mixtures, stems = data\n            zr = np.zeros((freq_bins,tframe))\n            if \"bass\" not in stems : \n                stems[\"bass\"] = torch.from_numpy(zr).to(device)\n            if \"vocal\" not in stems : \n                stems[\"vocal\"] = torch.from_numpy(zr).to(device)\n            if \"drums\" not in stems : \n                stems[\"drums\"] = torch.from_numpy(zr).to(device)\n            if \"other\" not in stems : \n                stems[\"other\"] = torch.from_numpy(zr).to(device)\n            #for s in stems.keys() : stems[s] = stems[s].unsqueeze(0) \n            outputs = model(mixtures)\n        \n            y1 = torch.tensor(avg_losses[i-1][0] / avg_losses[i-2][0] if i>1 else 1.)\n            y2 = torch.tensor(avg_losses[i-1][1] / avg_losses[i-2][1] if i>1 else 1.)\n            y3 = torch.tensor(avg_losses[i-1][2] / avg_losses[i-2][2] if i>1 else 1.)\n            y4 = torch.tensor(avg_losses[i-1][3] / avg_losses[i-2][3] if i>1 else 1.)\n            y = torch.tensor([y1,y2,y3,y4])/T\n            softmx = F.softmax(y,dim=0)\n            zero_mask = softmx==0.0\n            softmx[zero_mask] = 1e-9\n            print(\"softmx: \"+str(softmx))\n            dwa = K * softmx\n            loss1=loss_fn(outputs[\"vocal_stem\"], stems[\"vocal\"])\n            loss2=loss_fn(outputs[\"bass_stem\"], stems[\"bass\"])\n            loss3=loss_fn(outputs[\"drums_stem\"], stems[\"drums\"])\n            loss4=loss_fn(outputs[\"other_stem\"], stems[\"other\"])\n            loss=dwa[0]*loss1+dwa[1]*loss2+dwa[2]*loss3+dwa[3]*loss4\n            weightless_loss=[loss1.item(),loss2.item(),loss3.item(),loss4.item()]\n            testing_losses+=[weightless_loss]\n            current_avg = [sum(testing_losses[x][k] for x in range(i+1))/(i+1) for k in range(4)]\n            print(\"dwa: \"+str(dwa))\n            print('weightless_loss: '+str(weightless_loss))\n            print(\"current_avg: \"+ str(current_avg))\n            current_avg = [x  if x!=0.0 else 1e-9 for x in current_avg]\n            avg_losses += [current_avg]\n            '''\n            loss = multi_task_loss([outputs[\"vocal_stem\"],outputs[\"bass_stem\"],outputs[\"drums_stem\"],outputs[\"other_stem\"]], \n                               [stems[\"vocal\"],stems[\"bass\"],stems[\"drums\"],stems[\"other\"]])\n            '''\n            running_vloss += loss.item()\n            '''\n            o_vocal = librosa.griffinlim(stems[\"vocal\"].detach().cpu().numpy())\n            o_bass = librosa.griffinlim(stems[\"bass\"].detach().cpu().numpy())\n            o_drums = librosa.griffinlim(stems[\"drums\"].detach().cpu().numpy())\n            o_other = librosa.griffinlim(stems[\"other\"].detach().cpu().numpy())\n            \n            y_vocal = librosa.griffinlim(outputs[\"vocal_stem\"].detach().cpu().numpy())\n            y_bass = librosa.griffinlim(outputs[\"bass_stem\"].detach().cpu().numpy())\n            y_drums = librosa.griffinlim(outputs[\"drums_stem\"].detach().cpu().numpy())\n            y_other = librosa.griffinlim(outputs[\"other_stem\"].detach().cpu().numpy())\n            reference_sources = np.concatenate((np.expand_dims(o_vocal, axis=0), \n                                                np.expand_dims(o_bass, axis=0),\n                                                np.expand_dims(o_drums, axis=0),\n                                                np.expand_dims(o_other, axis=0)\n                                               ), axis=0)\n            estimated_sources = np.concatenate((np.expand_dims(y_vocal, axis=0), \n                                                np.expand_dims(y_bass, axis=0),\n                                                np.expand_dims(y_drums, axis=0),\n                                                np.expand_dims(y_other, axis=0)\n                                               ), axis=0)\n            (sdr, _, _, _) = mir_eval.separation.bss_eval_sources(reference_sources, estimated_sources, False)\n            \n            running_sdr+=sdr\n            '''\n            print(f\"testing batch {i+1}, loss: {loss.item()}\")\n\n        avg_vloss = running_vloss / (i + 1)\n        losses = 'LOSS epoch {}: train {} | valid {}'.format(epoch,avg_loss, avg_vloss)\n        print(losses)\n        avg_loss_history+=[losses]\n        scheduler.step(avg_vloss)\n        \n        # Track best performance, and save the model's state\n        if avg_vloss < best_vloss :\n            print(\"new best found, saving checkpoint\")\n            best_checkpoint_path = '/kaggle/working/model_kaggle_{}_{}'.format(timestamp, epoch)\n            best_vloss = avg_vloss\n            torch.save({\n                    'epoch': epoch,\n                    'model_state_dict': model.state_dict(),\n                    'optimizer_state_dict': optimizer.state_dict(),\n                    'loss_state_dict': multi_task_loss.state_dict(),\n                    'scheduler_state_dict': scheduler.state_dict()\n                    }, best_checkpoint_path)\n            #torch.save(model.state_dict(), model_path)\n        else : \n            print(\"not the best compared to last\")\nprint(\"\\n\".join(avg_loss_history))","metadata":{"execution":{"iopub.status.busy":"2024-03-18T08:30:20.935310Z","iopub.status.idle":"2024-03-18T08:30:20.935762Z","shell.execute_reply.started":"2024-03-18T08:30:20.935522Z","shell.execute_reply":"2024-03-18T08:30:20.935542Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.cuda.empty_cache()\ngc.collect()\n\nmodel2 = UNet().to(device)\nstate_dict = torch.load(best_checkpoint_path)\nprint(\"loaded for inference: \"+best_checkpoint_path)\nmodel2.load_state_dict(state_dict['model_state_dict'])\nmodel2.eval()\n\nclient_wav = \"/kaggle/input/musics/shape_of_you.mp3\"\nwav_name = client_wav.split(\"/\")[-1].split(\".\")[0]\naudio, sr = librosa.load(client_wav)\nS = np.abs(librosa.stft(audio))\nmagnitude, phase = librosa.magphase(S)\nphase = phase[:freq_bins,:tframe]\nmixture = torch.from_numpy(magnitude[:freq_bins,:tframe])\nmixture = mixture.to(device)\nto_pad = tframe-mixture.shape[1]\nif to_pad<0 :\n    if to_pad%2==0 :\n        mixture = F.pad(mixture,(to_pad//2,to_pad//2),mode='constant',value=0)\n    else :\n        mixture = F.pad(mixture,(to_pad//2,to_pad//2+1),mode='constant',value=0)\nmixture = mixture.unsqueeze(0)\noutputs = model2(mixture)\nbass = outputs[\"vocal_stem\"].detach().cpu().numpy()*phase\ndrums = outputs[\"bass_stem\"].detach().cpu().numpy()*phase\nother = outputs[\"drums_stem\"].detach().cpu().numpy()*phase\nvocal = outputs[\"other_stem\"].detach().cpu().numpy()*phase\n\ntorch.cuda.empty_cache()\ngc.collect()\n\nprint(bass.shape)\nprint(drums.shape)\nprint(bass.shape)\nprint(other.shape)\n\ny_inv_bass = np.ravel(librosa.griffinlim(bass))\ny_inv_drums = np.ravel(librosa.griffinlim(drums))\ny_inv_other = np.ravel(librosa.griffinlim(other))\ny_inv_vocal = np.ravel(librosa.griffinlim(vocal))\n\nprint(\"converted to wav\")\n\nsf.write(\"/kaggle/working/\"+wav_name+\"_bass.wav\", y_inv_bass, sr)\nsf.write(\"/kaggle/working/\"+wav_name+\"_drums.wav\", y_inv_drums, sr)\nsf.write(\"/kaggle/working/\"+wav_name+\"_other.wav\", y_inv_other, sr)\nsf.write(\"/kaggle/working/\"+wav_name+\"_vocal.wav\", y_inv_vocal, sr)\n\nprint(\"end\")","metadata":{"execution":{"iopub.status.busy":"2024-03-18T08:30:20.936858Z","iopub.status.idle":"2024-03-18T08:30:20.937374Z","shell.execute_reply.started":"2024-03-18T08:30:20.937089Z","shell.execute_reply":"2024-03-18T08:30:20.937107Z"},"trusted":true},"execution_count":null,"outputs":[]}]}